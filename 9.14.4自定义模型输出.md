### 14.4 自定义模型的输出
在14.3节中，我们完成了模型的微调（Fine - Tuning）训练过程，本节对训练结果进行输出。相对于传统的输出过程，GPT系列的输出更加复杂。

#### 14.4.1 GPT输出的结构
首先需要注意的是，GPT输出直观上并不是一种对称结构，一般结构如图14 - 10所示。

![image](https://github.com/user-attachments/assets/d6dbdcb8-8404-477a-b771-e8bd457392f8)


| 代预测输入端 -> | CLS | 你 | 好 | 人 | 工 |
| ---- | ---- | ---- | ---- | ---- | ---- |
| 模型输出端 -> | 好 | 的 | 发 | 和 | 事 | 智 |

图14 - 10 一般结构

这一点在14.1节已经介绍过了。这种输出方式的好处在于，模型只需要根据前文输出下一个字符即可，无须对整体的结果进行调整。

基于这种结果的生成方案，对于生成的字符，我们只需要循环地将最终生成的结果接入原有输入数据即可，如图14 - 11所示。

| CLS | 你 | 好 | 人 | 工 |
| ---- | ---- | ---- | ---- | ---- |
| 好 | 的 | 发 | 和 | 是 | 智 |
| CLS | 你 | 好 | 人 | 工 | 智 |
| 的 | 去 | 乐 | 意 | 买 | 法 | 能 |

图14 - 11 循环地将最终生成的结果接入原有输入数据

此方案的循环可以简单地用如下代码完成：
```python
import torch
from transformers import BertTokenizer
tokenizer = BertTokenizer.from_pretrained("uer/gpt2-chinese-cluecorpussmall")
from moudle import model

gpt_model = model.GPT2()

inputs_text = "你说"
input_ids = tokenizer.encode(inputs_text)
input_ids = input_ids[:-1] #这里转换成了list系列的ID
for _ in range(20):
    _input_ids = torch.tensor([input_ids],dtype=int)
    outputs = gpt_model(_input_ids)
    result = torch.argmax(outputs[0][-1],dim=-1)
    next_token = result.item()
    input_ids.append(next_token)

result = tokenizer.decode(input_ids, skip_special_tokens=True)
print(result)
```
打印结果请读者自行尝试。
下面我们继续对这段输出代码进行分析，既然是额外地使用循环输出对模型进行输出预测，能否将输出结果直接加载到我们自定义的GPT - 2模型内部？答案是可以的，带有自定义输出函数的GPT - 2模型如下（其中用到的temperature与topK在14.4.2节讲解）：
```python
import torch
from torch.nn.parameter import Parameter
from transformers import BertTokenizer, GPT2Model, TextGenerationPipeline
tokenizer = BertTokenizer.from_pretrained("uer/gpt2-chinese-cluecorpussmall")

class GPT2(torch.nn.Module):
    def __init__(self):
        super().__init__()
        #with model.no_grad():
        self.model = GPT2Model.from_pretrained("uer/gpt2-chinese-cluecorpussmall")
        self.lm_head = torch.nn.Linear(768,21128,bias=False)
        weight = torch.load("./dataset/lm_weight.pth")
        self.lm_head.weight = Parameter(weight)
        self.value_layer = torch.nn.Sequential(torch.nn.Linear(768,1),torch.nn.Tanh(),
torch.nn.Dropout(0.1))

    def forward(self,token_inputs):
        embedding = self.model(token_inputs)
        embedding = embedding["last_hidden_state"]
        embedding = torch.nn.Dropout(0.1)(embedding)
        logits = self.lm_head(embedding)
        return logits

    @torch.no_grad()
    def generate(self, continue_building_sample_num, prompt_token=None, temperature=1., top_p=0.95):
        """
        :param continue_building_sample_num: 这个参数指的是在输入的prompt_token后再输出多少个字符
        :param prompt_token: 这个是需要转换成Token的内容，这里需要输入一个list
        :param temperature: 
        :param top_k: 
        :return: 输出一个Token序列
        用法:
        """
        # prompt_token_new = prompt_token[:-1]#使用这行代码，在生成的Token中没有102分隔符
        prompt_token_new = list(prompt_token) # 使用这行代码，在生成的Token中包含102分隔符
        for i in range(continue_building_sample_num):
            _token_inp = torch.tensor([prompt_token_new]).to("cuda")
            logits = self.forward(_token_inp)
            logits = logits[:, -1, :]
            probs = torch.softmax(logits / temperature, dim=-1)
            next_token = self.sample_top_p(probs, top_p) # 预设的top_p = 0.95
            next_token = next_token.reshape(-1)
            prompt_token_new.append(next_token.item()) # 这是把Token从tensor转换成普通char, tensor -> list

        # text_context = tokenizer.decode(prompt_token, skip_special_tokens=True)
        return prompt_token_new

    def sample_top_p(self, probs, p):
        probs_sort, probs_idx = torch.sort(probs, dim=-1, descending=True)
        probs_sum = torch.cumsum(probs_sort, dim=-1)
        mask = probs_sum - probs_sort > p
        probs_sort[mask] = 0.0
        probs_sort.div_(probs_sort.sum(dim=-1, keepdim=True))
        next_token = torch.multinomial(probs_sort, num_samples=1)
        next_token = torch.gather(probs_idx, -1, next_token)
        return next_token
```
此时完整的模型输出如下：
```python
from transformers import BertTokenizer
tokenizer = BertTokenizer.from_pretrained("uer/gpt2-chinese-cluecorpussmall")
from moudle import model
gpt_model = model.GPT2()
gpt_model.to("cuda")
inputs_text = "酒店"
input_ids = tokenizer.encode(inputs_text)

for _ in range(10):
    prompt_token = gpt_model.generate(20,prompt_token=input_ids)
    result = tokenizer.decode(prompt_token, skip_special_tokens=True)
    print(result)
```
最终的打印结果请读者自行尝试。

#### 14.4.2 创造性参数temperature与采样个数topK

本小节讲解一下GPT模型中的temperature与topK这两个参数。对于生成模型来说，temperature可以认为是模型的创造性参数，即temperature值越大，模型的创造性越强，但生成效果不稳定；temperature值越小，则模型的稳定性越强，生成效果越稳定。

而topK的作用是挑选概率最高的k个Token作为候选集。若k值为1，则答案唯一；若topK为0，则该参数不起作用。

1. **temperature参数**

模型在数据生成的时候，会通过采样的方法增加文本生成过程中的随机性。文本生成是根据概率分布情况来随机生成下一个单词的。例如，已知单词[a, b, c]的生成概率分别是[0.1, 0.3, 0.6]，接下来生成c的概率就会比较大，生成a的概率就会比较小。

但如果按照全体词的概率分布来进行采样，还是有可能生成低概率的单词的，导致生成的句子出现语法或语义错误。通过在Softmax函数中加入temperature参数强化顶部词的生成概率，在一定程度上可以解决这一问题。

\[ p(i)=\frac{e^{\frac{z_i}{t}}}{\sum_{i}^{K} e^{\frac{z_i}{t}}} \]

在上述公式中，当t<1时，将会增加顶部词的生成概率，且t越小，越倾向于按保守的方法生成下一个词；当t>1时，将会增加底部词的生成概率，且t越大，越倾向于从均匀分布中生成下一个词。图14 - 12模拟每个字母生成的概率，观察t值大小对概率分布的影响，如图14 - 12所示。

|图14 - 12 模拟每个字母生成的概率|
|----|
| 图中展示了不同t值下概率分布的柱状图，t分别取0.1、0.5、1、2  |


这样做的好处在于生成的文本具有多样性和随机性，但是同时对t值的选择需要依赖模型设计人员的经验或调参。

![image](https://github.com/user-attachments/assets/a07975d1-bfd0-4e08-bdc6-6902c9aa1b79)


下面使用NumPy实现temperature值的设置，代码如下：
```python
def temperature_sampling(prob, T=0.2):
    def softmax(z):
        return np.exp(z) / sum(np.exp(z))
    log_prob = np.log(prob)
    reweighted_prob = softmax(log_prob / T)
    sample_space = list(range(len(prob)))
    original_sample = np.random.choice(sample_space, p=prob)
    temperature_sample = np.random.choice(list(range(len(prob))), p=reweighted_prob)
    return temperature_sample
```
2. **topK参数**
即使我们设置了temperature参数，选取了合适的t值，还是会有较低的可能性生成低概率的单词。因此，需要额外增加一个参数来确保低概率的词不会被选择到。

应用topK，可以根据概率分布情况预先挑选出一部分概率高的单词，然后对这部分单词进行采样，从而避免低概率词的出现。

topK是直接挑选概率最高的k个单词，然后重新根据Softmax计算这k个单词的概率，再根据概率分布情况进行采样，生成下一个单词。采样还可以选用temperature方法。此方法的NumPy实现如下：
```python
def top_k(prob, k=5):
    def softmax(z):
        return np.exp(z) / sum(np.exp(z))

    topk = sorted([(p, i) for i, p in enumerate(prob)], reverse=True)[:k]
    k_prob = [p for p, i in topk]
    k_prob = softmax(np.log(k_prob))
    k_idx = [i for p, i in topk]
    return k_idx, k_prob, np.random.choice(k_idx, p=k_prob)
```
采用topK的方案可以避免低概率词的生成。但是与temperature一样，k值的选择需要依赖于经验或调参。比如，在较狭窄的分布中，选取较小的k值；在较宽广的分布中，选取较大的k值。

### 14.5 本章小结

本章主要介绍了GPT系列中最重要的一个模型——GPT - 2，这个模型可以说在真正意义上开启了只具有解码器的文本生成任务。GPT - 2后续的GPT - 3和第15章所要介绍的ChatGPT实战训练都是在其基础上应运而生的。

本章是ChatGPT的起始章节，详细介绍了GPT - 2模型的训练与自定义的方法，还讲解了使用切分的方法对模型进行分布存档和训练，这实际上是为第15章ChatGPT的使用打下基础。

第15章讲解的ChatGPT会以GPT - 2为模板，使用RLHF系统完成ChatGPT的训练。 

