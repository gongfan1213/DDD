### 2. GPT - 2模型的Model类

下面介绍GPT - 2模型的Model类，其作用是将Block组合在一起，构成一个完整的数据处理模块，并将数据传输到任务分类模块中，代码如下：
```python
class GPT2Model(nn.Module):
    def __init__(self, config):
        super(GPT2Model, self).__init__()
        self.n_layer = config.n_layer
        self.n_embd = config.n_embd
        self.n_vocab = config.vocab_size
        self.wte = nn.Embedding(config.vocab_size, config.n_embd)
        self.wpe = nn.Embedding(config.n_positions, config.n_embd)
        block = Block(config.n_ctx, config, scale=True)
        self.h = nn.ModuleList([copy.deepcopy(block) for _ in range(config.n_layer)])
        self.ln_f = LayerNorm(config.n_embd, eps=config.layer_norm_epsilon)

    def set_embeddings_weights(self, model_embeddings_weights):
        embed_shape = model_embeddings_weights.shape
        self.decoder = nn.Linear(embed_shape[1], embed_shape[0], bias=False)
        self.decoder.weight = model_embeddings_weights  # Tied weights

    def forward(self, input_ids, position_ids=None, token_type_ids=None, past=None):
        if past is None:
            past_length = 0
            past = [None] * len(self.h)
        else:
            past_length = past[0][0].size(-2)
        if position_ids is None:
            position_ids = torch.arange(past_length, input_ids.size(-1) + past_length,
                                        dtype=torch.long, device=input_ids.device)
            position_ids = position_ids.unsqueeze(0).expand_as(input_ids)
        input_shape = input_ids.size()
        input_ids = input_ids.view(-1, input_ids.size(-1))
        position_ids = position_ids.view(-1, position_ids.size(-1))

        inputs_embeds = self.wte(input_ids)
        position_embeds = self.wpe(position_ids)
        if token_type_ids is not None:
            token_type_ids = token_type_ids.view(-1, token_type_ids.size(-1))
            token_type_embeds = self.wte(token_type_ids)
        else:
            token_type_embeds = 0
        hidden_states = inputs_embeds + position_embeds + token_type_embeds
        presents = []
        for block, layer_past in zip(self.h, past):
            hidden_states, present = block(hidden_states, layer_past)
            presents.append(present)
        hidden_states = self.ln_f(hidden_states)
        output_shape = input_shape + (hidden_states.size(-1),)
        return hidden_states.view(*output_shape), presents
```

### 3. GPT - 2模型的任务分类
GPT2LMHeadModel类用于来进行自回归训练，其可以传入labels张量来计算自回归交叉熵损失值loss，继而利用自回归交叉熵损失值loss来优化整个GPT - 2模型。


虽然GPT2LMHeadModel类可以用来进行自回归训练，但它也可以在下游任务或其他情景中被使用，此时便不需要为GPT2LMHeadModel类传入labels张量。
```python
class GPT2LMHead(nn.Module):
    def __init__(self, model_embeddings_weights, config):
        super(GPT2LMHead, self).__init__()
        self.n_embd = config.n_embd
        self.set_embeddings_weights(model_embeddings_weights)

    def set_embeddings_weights(self, model_embeddings_weights):
        embed_shape = model_embeddings_weights.shape
        self.decoder = nn.Linear(embed_shape[1], embed_shape[0], bias=False)
        self.decoder.weight = model_embeddings_weights  # Tied weights

    def forward(self, hidden_state):
        # Truncated Language modeling logits (we remove the last token)
        h_trunc = h[:, :-1].contiguous().view(-1, self.n_embd)
        lm_logits = self.decoder(hidden_state)
        return lm_logits


class GPT2LMHeadModel(nn.Module):
    def __init__(self, config):
        super(GPT2LMHeadModel, self).__init__()
        self.transformer = GPT2Model(config)
        self.lm_head = GPT2LMHead(self.transformer.wte.weight, config)

    def set_tied(self):
        """ Make sure we are sharing the embeddings """
        self.lm_head.set_embeddings_weights(self.transformer.wte.weight)

    def forward(self, input_ids, position_ids=None, token_type_ids=None, lm_labels=None, past=None):
        hidden_states, presents = self.transformer(input_ids, position_ids,
                                                   token_type_ids, past)
        lm_logits = self.lm_head(hidden_states)
        if lm_labels is not None:
            loss_fct = nn.CrossEntropyLoss(ignore_index=-1)
            loss = loss_fct(lm_logits.view(-1, lm_logits.size(-1)), lm_labels.view(-1))
            return loss
        return lm_logits, presents
```

### 4. GPT - 2模型的整体参数和结构
最终通过设定GPT - 2模型的整体参数可以完成模型的构建，此时对于参数的设置，可以通过建立一个config类（GPT2Config）的方法来实现，代码如下：
```python
class GPT2Config(object):
    def __init__(
        self,
        vocab_size=1024,  # 字符个数
        n_positions=1024,  # 位置Embedding的维度
        n_ctx=1024,  # 注意力中的Embedding的维度
        n_embd=768,  # GPT模型维度
        n_layer=12,  # GPT中Block的层数
        n_head=12,  # GPT中的注意力头数
        layer_norm_epsilon=1e-5,
        initializer_range=0.02,
    ):
        self.vocab_size = vocab_size
        self.n_ctx = n_ctx
        self.n_positions = n_positions
        self.n_embd = n_embd
        self.n_layer = n_layer
        self.n_head = n_head
        self.layer_norm_epsilon = layer_norm_epsilon
        self.initializer_range = initializer_range
```
通过使用这个config类（GPT2Config）可以完整地构建GPT - 2的模型。
```python
import gpt2_config
config = gpt2_config.GPT2Config()
# GPT2LMHeadModel
gpt2_model = GPT2LMHeadModel(config)
token = torch.randint(1, 128, (2, 100))
logits, presents = gpt2_model(token)
print(logits.shape)
```

#### 14.1.3 GPT - 2模型输入输出格式的实现
下面需要做的是GPT - 2模型输入和输出格式的处理，我们在介绍解码器时，着重讲解了训练过程的输入输出，相信读者已经应该掌握了“错位”输入方法。

相对于完整Transformers架构的翻译模型，GPT - 2的输入和输出与其类似，且更为简单，即采用完整相同的输入序列，而仅进行错位即可。例如，我们需要输入一句完整的话“你好人工智能！”

完整的表述如图14 - 6所示。


![image](https://github.com/user-attachments/assets/3046c446-7d9a-4807-b903-0252234c1cc5)


但是此时不能将其作为单独的输入端或者输出端输入模型中进行训练，而是需要对其进行错误表示，如图14 - 7所示。

可以看到，此时我们构建的数据输入和输出虽然长度相同，但是在位置上是错位的，通过在前端出现的文本来预测下一个位置出现的字或者词。

另外，需要注意的是，在使用训练后的GPT - 2进行下一个真正文本预测时，相对于前面学习的编码器文本的输出格式，输出的内容很有可能相互之间没有关系，如图14 - 8所示。


可以看到，这段模型输出的前端部分和输入文本部分毫无关系（浅色部分），而仅对输出的下一个字符进行预测和展示。

而对于进行完整语句的处理，则可以通过滚动循环的形式不断地对下一个字符进行预测，从而完成一个完整语句的输出。

这段内容实现的示例代码如下：


![image](https://github.com/user-attachments/assets/b8f23dcd-dbba-46de-9b03-46084c16c06e)


```python
import numpy as np
from tqdm import tqdm
import torch
import einops.layers.torch as elt
from tqdm import tqdm
import torch
import numpy as np
# 下面使用Huggingface提供的tokenizer
from transformers import BertTokenizer
tokenizer = BertTokenizer.from_pretrained("uer/gpt2-chinese-cluecorpussmall")
token_list = []
with open("./dataset/ChnSentiCorp.txt", mode="r", encoding="UTF-8") as emotion_file:
    for line in emotion_file.readlines():
        line = line.strip().split(",")
        text = "".join(line[1:])
        inputs = tokenizer(text, return_tensors='pt')
        token = input_ids = inputs["input_ids"]
        attention_mask = inputs["attention_mask"]
        for id in token[0]:
            token_list.append(id.item())
token_list = torch.tensor(token_list * 5)


class TextSamplerDataset(torch.utils.data.Dataset):
    def __init__(self, data, seq_len):
        super().__init__()
        self.data = data
        self.seq_len = seq_len

    def __getitem__(self, index):
        rand_start = torch.randint(0, self.data.size(0) - self.seq_len, (1,))
        full_seq = self.data[rand_start : rand_start + self.seq_len + 1].long()
        return full_seq[:-1], full_seq[1:]

    def __len__(self):
        return self.data.size(0) // self.seq_len
```
虽然我们实现了GPT - 2的基本模型，但是在此并没有完成训练一个可以进行文本输出的GPT - 2模型，有兴趣的读者可以自行尝试。 
