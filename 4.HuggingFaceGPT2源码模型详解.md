### 14.2 Hugging Face GPT - 2模型源码模型详解
14.1节介绍了GPT - 2模型的基本架构，并详细讲解了GPT - 2模型的输入输出格式，但是并没有使用GPT - 2模型进行训练，这是因为相对于已训练好的GPT - 2模型，普通用户基本上不可能训练出一个具有全面水平的GPT - 2，因此我们将从Huggingface库中的GPT - 2模型源码层面深入理解GPT - 2模型的结构。


#### 14.2.1 GPT2LMHeadModel类和GPT2Model类详解

Huggingface官方给出了一个调用GPT2LMHeadModel类来使用GPT - 2模型的例子，代码如下：
```python
#!/usr/bin/env Python
# coding=utf-8
from transformers import GPT2LMHeadModel, GPT2Tokenizer
import torch
# 初始化GPT-2模型的Tokenizer类
tokenizer = GPT2Tokenizer.from_pretrained("gpt2")
# 初始化GPT-2模型，此处以初始化GPT2LMHeadModel()类的方式调用GPT-2模型
model = GPT2LMHeadModel.from_pretrained('gpt2')
# model.config.use_return_dict = None
# print(model.config.use_return_dict)
# GPT模型第一次迭代输入的上下文内容，将其编码以序列化
# 同时，generated也用来存储GPT2模型所有迭代生成的Token索引
generated = tokenizer.encode("The Manhattan bridge")
# 将序列化后的第一次迭代的上下文内容转化为PyTorch中的tensor形式
context = torch.tensor([generated])
# 第一次迭代时还没有past_key_values元组
past_key_values = None
for i in range(30):
    """
    此时模型model返回的output为CausalLMOutputWithPastAndCrossAttentions类，
    模型返回的logits和past_key_values对象为其中的属性
    CausalLMOutputWithPastAndCrossAttentions(
        loss=loss,
        logits=lm_logits,
        past_key_values=transformer_outputs.past_key_values,
        hidden_states=transformer_outputs.hidden_states,
        attentions=transformer_outputs.attentions,
        cross_attentions=transformer_outputs.cross_attentions,
    )
    """
    output = model(context, past_key_values=past_key_values)
    past_key_values = output.past_key_values
    # 此时获取GPT2模型的输出结果hidden_states张量中第二维度最后一个元素的argmax值全得出的
    # argmax值即为此次GPT2模型迭代
    # 计算生成的下一个token。注意，此时若是第一次迭代，则输出结果hidden_states张量的形状为
    # (batch_size, seq_len, n_state)；
    # 此时若是第二次及之后的迭代，则输出结果hidden_states张量的形状为(batch_size, 1, n_state)，
    # all_head_size=n_state=nx*n_embd=768
    token = torch.argmax(output.logits[..., -1, :])
    # 将本次迭代生成的Token的张量变为二维张量，以作为下一次GPT2模型迭代计算的上下文context
    context = token.unsqueeze(0)
    # 将本次迭代计算生成的token的序列索引变为列表存入generated
    generated += [token.tolist()]

    # 将generated中所有的Token的索引转化为Token字符
    sequence = tokenizer.decode(generated)
    sequence = sequence.split(".")[:-1]
    print(sequence)
```

从上述代码中可以看出，context为每次迭代输入模型中的input_ids张量；past_key_values为GPT - 2模型中12层Block模块计算后得到的存储12个present张量的presents元组，每个present张量存储着past_key张量与这次迭代的key张量合并后的新key张量，以及past_value张量与这次迭代的value张量合并后的新value张量，一个present张量的形状为(2, batch_size, num_head, sql_len + 1, head_features)，其中key张量、past_key张量、value张量、past_value张量、present张量皆是在Attention模块中用于计算的。

past_key_values是GPT - 2中最重要的机制，其可以防止模型在文本生成任务中重新计算上一次迭代中已经计算好的上下文的值，大大提高了模型在文本生成任务中的计算效率。但要特别注意，在第一次迭代时，由于不存在上一次迭代返回的past_key_values值，因此第一次迭代时past_key_values的值为None。

实际上，在目前大多数可用于文本生成任务的预训练模型中，都存在past_key_values机制，比如Google的T5模型、Facebook的BART模型等，因此理解了GPT - 2模型中的past_key_values机制，对理解T5、BART等模型也会有帮助。

GPT2LMHeadModel类不仅可以用来进行自回归训练（传入labels），也可以用来执行下游任务，如文本生成等，GPT - 2源码中GPT2LMHeadModel类的部分代码如下：

```python
class GPT2LMHeadModel(GPT2PreTrainedModel):
    _keys_to_ignore_on_load_missing = [r"h\.\d+\.attn\.masked_bias",
                                       r"lm_head\.weight"]
    def __init__(self, config):
        super().__init__(config)
        # 初始化GPT2Model(config)类
        self.transformer = GPT2Model(config)
        # self.lm_head为将GPT2Model(config)计算输出的hidden_states张量的最后一个维度由768维
        # (config.n_embd)投影为词典大小维度(config.vocab_size)的输出层，此时hidden_states张量的形状将会由
        # (batch_size, 1, n_embd)投影变为lm_logits张量的(batch_size, 1, vocab_size)
        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)
        # 重新初始化权重矩阵
        self.init_weights()

    def get_output_embeddings(self):
        return self.lm_head

    def prepare_inputs_for_generation(self, input_ids, past=None, **kwargs):
        token_type_id = kwargs.get("token_type_ids", None)
        # only last token for inputs_ids if past is defined in kwargs
        if past:
            input_ids = input_ids[:, -1].unsqueeze(-1)
            if token_type_id is not None:
                token_type_ids = token_type_ids[:, -1].unsqueeze(-1)
        attention_mask = kwargs.get("attention_mask", None)
        position_ids = kwargs.get("position_ids", None)
        if attention_mask is not None and position_ids is None:
            # create position ids on the fly for batch generation
            position_ids = attention_mask.long().cumsum(-1) - 1
            position_ids.masked_fill_(attention_mask == 0, 1)
            if past:
                position_ids = position_ids[:, -1].unsqueeze(-1)
        else:
            position_ids = None
        return {
            "input_ids": input_ids,
            "past_key_values": past,
            "use_cache": kwargs.get("use_cache"),
            "position_ids": position_ids,
            "attention_mask": attention_mask,
            "token_type_ids": token_type_ids,
        }

    @add_start_docstrings_to_model_forward(GPT2_INPUTS_DOCSTRING)
    @add_code_sample_docstrings(
        tokenizer_class=_TOKENIZER_FOR_DOC,
        checkpoint="gpt2",
        output_type=CausalLMOutputWithPastAndCrossAttentions,
        config_class=_CONFIG_FOR_DOC,
    )
    def forward(
        self,
        input_ids=None,
        past_key_values=None,
        attention_mask=None,
        token_type_ids=None,
        position_ids=None,
        head_mask=None,
        inputs_embeds=None,
        encoder_hidden_states=None,
        encoder_attention_mask=None,
        labels=None,
        use_cache=None,
        output_attentions=None,
        output_hidden_states=None,
        return_dict=None,
    ):
        r"""
        labels (:obj:`torch.LongTensor` of shape :obj:`(batch_size, sequence_length)`, `optional`):
            Labels for language modeling. Note that the labels **are shifted** inside the model, i.e. you can set
            ``labels = input_ids`` Indices are selected in ``[-100, 0, ..., config.vocab_size]`` All labels set to
            ``-100`` are ignored (masked), the loss is only computed for labels in ``[0, ..., config.vocab_size]``
        """
        return_dict = return_dict if return_dict is not None else self.config.use_return_dict
        # 此时返回的transformer_outputs中为
        # <1> 第一个值为GPT-2模型中经过12层Block模块计算后得到的最终hidden_states张量
        # 形状为(batch_size, 1, n_state)，all_head_size=n_state=nx*n_embd=768
        # <2> 第二个值为GPT-2模型中12层Block模块计算后得到的存储12个present张量的presents元组，
        # 每个present张量存储着past_key张量与这次迭代的key张量合并后的新key张量，以及past_value张量与这次迭代的
        # value张量合并后的新value张量
        # 一个present张量形状为(2, batch_size, num_head, sql_len+1, head_features)
        # <3> 若output_hidden_states为True，则第三个值为GPT-2模型中12层Block模块计算后得到的
        # 存储12个隐藏状态张量hidden_state的all_hidden_states元组
        # <4> 若output_attentions为True，则第四个值为GPT-2模型中12层Block模块计算后得到的存储
        # 12个注意力分数张量w的all_self_attentions元组
        # <5> 若此时进行了Cross Attention计算，则第五个值为GPT-2模型中12层Block模块计算后得到的
        # 存储12个交叉注意力分数张量cross_attention的all_cross_attentions元组
        # 其中每个交叉注意力分数张量cross_attention形状为(batch_size,num_head,1,enc_seq_len)
        transformer_outputs = self.transformer(
            input_ids,
            past_key_values=past_key_values,
            attention_mask=attention_mask,
            token_type_ids=token_type_ids,
            position_ids=position_ids,
            head_mask=head_mask,
            inputs_embeds=inputs_embeds,
            encoder_hidden_states=encoder_hidden_states,
            encoder_attention_mask=encoder_attention_mask,
            use_cache=use_cache,
            output_attentions=output_attentions,
            output_hidden_states=output_hidden_states,
            return_dict=return_dict,
        )
        hidden_states = transformer_outputs[0]
        # self.lm_head()输出层将GPT2Model(config)计算输出的hidden_states张量的最后一个维度由
        # 768维(config.n_embd)投影为词典大小维度(config.vocab_size)的输出层，此时hidden_states张量的形状将
        # 会由(batch_size, 1, n_embd)投影变为lm_logits张量的(batch_size, 1, vocab_size)
        lm_logits = self.lm_head(hidden_states)
        loss = None
        # 若此时labels也输入GPT2LMHeadModel()类中，则会使用自回归的方式计算交叉熵损失
        # 即此时的shift_操作为将GPT2Model(config)计算输出的hidden_states张量的最后一个维度
        # 由768维(config.n_embd)投影为词典大小维度(config.vocab_size)所得到的lm_logits张量的切片
        # lm_logits[..., :-1, :].contiguous()，即config.vocab_size的lm_logits值
        # 形如labels[..., 1:].contiguous()的作用是将输入的labels张量进行切片，只保留第一个起始字符后的序列内容，
        # 因此利用(1, n-1)的lm_logits值与(2, n)的label值即可计算此时自回归训练的交叉熵损失值
        if labels is not None:
            # Shift so that tokens < n predict n
            shift_logits = lm_logits[..., :-1, :].contiguous()
            shift_labels = labels[..., 1:].contiguous()
            # Flatten the tokens
            loss_fct = nn.CrossEntropyLoss()
            loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)),
                            shift_labels.view(-1))
            # <1> 若loss不为None，则代表此时输入了labels张量，进行了自回归的交叉熵损失计算，此时第一个
            # 值为自回归交叉熵损失loss
            # <2> 第二个值将GPT2Model(config)计算输出的hidden_states张量的最后一个维度由768维
            # (config.n_embd)投影为词典大小维度(config.vocab_size)的lm_logits张量，其形状为(batch_size, 1,
            # vocab_size)
            # <3> 第三个值为GPT-2模型中12层Block模块计算后得到的存储12个present张量的presents元组，
            # 每个present张量存储着past_key张量与这次迭代的key张量合并后的新key张量，以及past_value张量与这次迭代的
            # value张量合并后的新value张量
            # 一个present张量形状为(2, batch_size, num_head, sql_len+1, head_features)
            # <4> 若output_hidden_states为True，则第四个值为GPT-2模型中12层Block模块计算后得到的
            # 存储12个隐藏状态张量hidden_state的all_hidden_states元组
            # <5> 若output_attentions为True，则第五个值为GPT-2模型中12层Block模块计算后得到的存储
            # 12个注意力分数张量w的all_self_attentions元组
            # <6> 若此时进行了Cross Attention计算，则第六个值为GPT-2模型中12层Block模块计算后得到的
            # 存储12个交叉注意力分数张量cross_attention的all_cross_attentions元组
            # 其中每个交叉注意力分数张量cross_attention形状为(batch_size,num_head,1,enc_seq_len)
        if not return_dict:
            output = (lm_logits,) + transformer_outputs[1:]
            return (loss,) + output if loss is not None else output
        return CausalLMOutputWithPastAndCrossAttentions(
            loss=loss,
            logits=lm_logits,
            past_key_values=transformer_outputs.past_key_values,
            hidden_states=transformer_outputs.hidden_states,
            attentions=transformer_outputs.attentions,
            cross_attentions=transformer_outputs.cross_attentions,
        )
```

GPT2LMHeadModel类中的代码可参考注释来理解。

从GPT2LMHeadModel类的代码中可以看出，其主体为调用GPT2Model类以及一个输出层self.lm_head，GPT2Model类用来进行12层Block的计算，而输出层self.lm_head则将GPT2Model类输出的最后一个Block层隐藏状态hidden_states张量的最后一个维度，由768维（config.n_embd）投影为词典大小（config.vocab_size），hidden_states张量经过输出层投影后即为lm_logits张量。

当使用GPT2LMHeadModel类进行自回归预训练时，其可以传入labels张量。当GPT2LMHeadModel类中使用GPT2Model类与输出层self.lm_head计算得出了最终的lm_logits值时，lm_logits张量便可以与传入的labels张量利用自回归的方式（取(1, n - 1)的lm_logits值与(2, n)的label值）来计算自回归交叉熵损失值loss。自回归交叉熵损失值loss便可以用来反向传播计算梯度，最终优化整个GPT - 2模型。

需要注意的是，此时代码中的config为transformers库configuration_gpt2模块中的GPT2Config类，GPT2Config类中保存了GPT - 2模型中的各种超参数，若在使用GPT - 2模型时需要修改某一超参数，则只需在传入GPT - 2模型的config（GPT2Config类）中修改对应的超参数即可。

GPT2Model类的代码如下：
```python
class GPT2Model(GPT2PreTrainedModel):
    def __init__(self, config):
        super().__init__(config)
        self.wte = nn.Embedding(config.vocab_size, config.n_embd)
        self.wpe = nn.Embedding(config.n_positions, config.n_embd)
        self.drop = nn.Dropout(config.embd_pdrop)
        self.h = nn.ModuleList([Block(config.n_ctx, config, scale=True) for _ in
                                range(config.n_layer)])
        self.ln_f = nn.LayerNorm(config.n_embd, eps=config.layer_norm_epsilon)
        self.init_weights()

    def get_input_embeddings(self):
        return self.wte

    def set_input_embeddings(self, new_embeddings):
        self.wte = new_embeddings

    def _prune_heads(self, heads_to_prune):
        """
        Prunes heads of the model. heads_to_prune: dict of {layer_num: list of heads to
        prune in this layer}
        """
        for layer, heads in heads_to_prune.items():
            self.h[layer].attn.prune_heads(heads)

       @add_start_docstrings_to_model_forward(GPT2_INPUTS_DOCSTRING)
    @add_code_sample_docstrings(
        tokenizer_class=_TOKENIZER_FOR_DOC,
        checkpoint="gpt2",
        output_type=BaseModelOutputWithPastAndCrossAttentions,
        config_class=_CONFIG_FOR_DOC,
    )
    def forward(
        self,
        input_ids=None,
        past_key_values=None,
        attention_mask=None,
        token_type_ids=None,
        position_ids=None,
        head_mask=None,
        inputs_embeds=None,
        encoder_hidden_states=None,
        encoder_attention_mask=None,
        use_cache=None,
        output_attentions=None,
        output_hidden_states=None,
        return_dict=None,
    ):
        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions
        output_hidden_states = output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states
        use_cache = use_cache if use_cache is not None else self.config.use_cache
        return_dict = return_dict if return_dict is not None else self.config.use_return_dict

        # input_ids与inputs_embeds只能输入一个，有input_ids便只需将input_ids输入嵌入层即可转
        # 换为类似inputs_embeds的张量，有inputs_embeds便不需要input_ids
        if input_ids is not None and inputs_embeds is not None:
            raise ValueError("You cannot specify both input_ids and inputs_embeds at the same time")

        # 下面确保输入的input_ids、token_type_ids、position_ids等张量的形状为正确的样式
        # <1> 若为模型第一次迭代，则此时input_ids、token_type_ids、position_ids等张量的正确形
        # 状为(batch_size, seq_len)
        # <2> 若为模型第二次及之后的迭代，则此时input_ids、token_type_ids、position_ids等张量
        # 的正确形状为(batch_size, 1)
        # 最后，将输入的input_ids、token_type_ids、position_ids等张量的形状保存到input_shape中
        elif input_ids is not None:
            input_shape = input_ids.size()
            input_ids = input_ids.view(-1, input_shape[-1])
            batch_size = input_ids.shape[0]
        elif inputs_embeds is not None:
            input_shape = inputs_embeds.size()[:-1]
            batch_size = inputs_embeds.shape[0]
        else:
            raise ValueError("You have to specify either input_ids or inputs_embeds")

        if token_type_ids is not None:
            token_type_ids = token_type_ids.view(-1, input_shape[-1])
        if position_ids is not None:
            position_ids = position_ids.view(-1, input_shape[-1])

        if past_key_values is None:
            past_length = 0
            # 若此时为GPT-2模型第一次迭代，则不存在上一次迭代返回的past_key_values列表(包含12个
            # Present的列表，也就是代码中的presents列表)，此时past_key_values列表为一个包含12个None值的列表
            past_key_values = [None] * len(self.h)
        else:
            past_length = past_key_values[0][0].size(-2)

        if position_ids is None:
            device = input_ids.device if input_ids is not None else inputs_embeds.device
            """
            <1> 若GPT2Model第一次迭代时GPT2Model的forward()函数中的past_key_values参数为
            None，此时past_length为0，hidden_states张量形状为(batch_size, seq_len, n_embd)，config的
            GPT2Config()类中的n_embd默认为768
            <2> 若为GPT2Model第二次及之后的迭代，此时past_length为上一次迭代时记录保存下来的
            past_key_values中张量的seq_len维度，而input_shape[-1] + past_length则等于seq_len + 1，因为在第
            二次及之后的迭代中，输入的文本编码(input_ids)的seq_len维度本身为1，即第二次及之后的迭代中每次只输入一
            个字的文本编码，此时position_ids张量形状为(batch_size, 1)
            """
            position_ids = torch.arange(past_length, input_shape[-1] + past_length,
                                        dtype=torch.long, device=device)
            position_ids = position_ids.unsqueeze(0).view(-1, input_shape[-1])

        # Attention mask
        # attention_mask张量为注意力遮罩张量，其让填充特殊符[PAD]处的注意力分数极小，
        # 其Embedding嵌入值基本不会在多头注意力聚合操作中被获取到
        if attention_mask is not None:
            assert batch_size > 0, "batch_size has to be defined and > 0"
            attention_mask = attention_mask.view(batch_size, -1)
            # 在这里基于输入的2D数据创建了一个4D注意力遮罩张量，大小为[batch_size, 1, 1,
            # to_seq_length]
            # 其作用是与输入的多头向量(batch_size,num_heads, from_seq_length, to_seq_length)
            # 进行叠加从而完成对掩码部分的遮罩
            attention_mask = attention_mask[:, None, None, :]

            # 此时设置的序号1的位置为保留的文本部分，而0序号的位置是对其中的内容进行遮罩，
            # 并使用-10000的值进行填充，其目的是在后续的softmax中忽略遮罩部分的计算
            attention_mask = attention_mask.to(dtype=self.dtype)  # fp16 compatibility
            attention_mask = (1.0 - attention_mask) * -10000.0

            # If a 2D ou 3D attention mask is provided for the cross-attention
            # we need to make broadcastable to [batch_size, num_heads, seq_length, seq_length]
            # 若此时有从编码器encoder中传入的编码器隐藏状态encoder_hidden_states，则获取编码器隐藏状
            # 态对应的attention_mask张量(encoder_attention_mask)
            if self.config.add_cross_attention and encoder_hidden_states is not None:
                encoder_batch_size, encoder_sequence_length, _ = encoder_hidden_states.size()
                encoder_hidden_shape = (encoder_batch_size, encoder_sequence_length)
                if encoder_attention_mask is None:
                    encoder_attention_mask = torch.ones(encoder_hidden_shape, device=device)
                else:
                    encoder_attention_mask = self.invert_attention_mask(encoder_attention_mask)
            else:
                encoder_attention_mask = None

        # Prepare head mask if needed
        # 1.0 in head_mask indicate we keep the head
        # attention_probs has shape bsz x n_heads x N x N
        # head_mask has shape n_layer x batch x n_heads x N x N
        # prune_heads()可结合https://github.com/huggingface/transformers/issues/850理解
        head_mask = self.get_head_mask(head_mask, self.config.n_layer)

        # 将input_ids、token_type_ids、position_ids等张量输入嵌入层self.wte()、self.wpe()
        # 中之后获取其嵌入形式张量
        # inputs_embeds、position_embeds与token_type_embeds
        if inputs_embeds is None:
            inputs_embeds = self.wte(input_ids)
        position_embeds = self.wpe(position_ids)
        hidden_states = inputs_embeds + position_embeds

        if token_type_ids is not None:
            token_type_embeds = self.wte(token_type_ids)
            hidden_states = hidden_states + token_type_embeds

        """
        <1> GPT2Model第一次迭代时GPT2Model的forward()函数中的past_key_values参数为
        None，此时past_length为0，hidden_states张量形状为(batch_size, seq_len, n_embd)，config的
        GPT2Config()类中的n_embd默认为768
        <2> 若为GPT2Model第二次及之后的迭代，此时past_length为上一次迭代时记录保存下来的
        past_key_values中张量的seq_len维度，而input_shape[-1] + past_length则等于seq_len + 1，因为在第
        二次及之后的迭代中，输入的文本编码(input_ids)的seq_len维度本身为1，即第二次及之后的迭代中每次只输入一
        个字的文本编码，此时hidden_states张量形状为(batch_size, 1, n_embd)，config的GPT2Config()类中的n_embd
        默认为768
        """
        hidden_states = self.drop(hidden_states)

        output_shape = input_shape + (hidden_states.size(-1),)

        # config对应的GPT2Config()类中的use_cache默认为True。
        presents = () if use_cache else None
        all_self_attentions = () if output_attentions else None
        all_cross_attentions = () if output_attentions and self.config.add_cross_attention else None
        all_hidden_states = () if output_hidden_states else None

        for i, (block, layer_past) in enumerate(zip(self.h, past_key_values)):
            """
            此处past_key_values元组中一共有12个元素(layer_past)，分别对应GPT-2模型中的12层
            Transformer_Block，每个layer_past都为模型上一次迭代中每个Transformer_Block保留下来的present张量，
            而每个present张量保存着Transformer_Block中Attention模块将本次迭代的key张量与上一次迭代中的
            past_key张量(layer_past[0])合并、将本次迭代的value张量与上一次迭代中的past_value张量(layer_past[1])
            合并所得的新的key张量与value张量，之后保存着本次迭代中12层Transformer_Block每一层返回的present张量
            的presents元组，便会被作为下一次迭代中的past_key_values元组输入下一次迭代的GPT-2模型中。新的key张量与
            value张量详细解析如下：
            """
            """
            第一次迭代时，query、key、value张量的seq_len维度处的维数就为seq_len而不是1，第二
            次之后seq_len维度的维数大小皆为'1'
            """
            """
            <1> 本次迭代中新的key张量
            此时需要通过layer_past[0].transpose(-2, -1)操作将past_key张量的形状变为
            (batch_size, num_head, head_features, seq_len)，而此时key张量的形状为(batch_size, num_head,
            head_features, 1)，这样下方就将past_key张量与key张量在最后一个维度(Dim=-1)处进行合并，这样就将
            当前Token的key部分加入past_key的seq_len部分了，以方便模型在后面预测新的token，此时新的key张量的形状为
            (batch_size, num_head, head_features, sql_len+1)，new_seq_len为sql_len+1
            <2> 本次迭代中新的value张量
            此时past_value(layer_past[1])不用变形，其形状为(batch_size, num_head, sql_len,
            head_features)，而此时value张量的形状为(batch_size, num_head, 1, head_features)，这样在下方就方
            便将past_value张量与value张量在倒数第二个维度(dim=-2)处进行合并，这样就将当前token的value部分加入了
            past_value的seq_len部分以方便模型在后面预测新的Token，此时新的value张量的形状为：(batch_size,
            num_head, sql_len+1, head_features)，new_seq_len为sql_len+1
            """
            if output_hidden_states:
                all_hidden_states = all_hidden_states + (hidden_states.view(*output_shape),)

            if getattr(self.config, "gradient_checkpointing", False):
                def create_custom_forward(module):
                    def custom_forward(*inputs):
                        # checkpointing only works with tuple returns, not with lists
                        return tuple(output for output in module(*inputs, use_cache, output_attentions))
                    return custom_forward
                outputs = torch.utils.checkpoint.checkpoint(
                    create_custom_forward(block),
                    hidden_states,
                    layer_past,
                    attention_mask,
                    head_mask[i],
                    encoder_hidden_states,
                    encoder_attention_mask,
                )
            else:
                # 此时返回的outputs列表中的元素为
                # <1> 第一个值为多头注意力聚合操作结果张量hidden_states输入前馈MLP层与残差连接之
                # 后得到的hidden_states张量(batch_size, 1, n_state)，all_head_size=n_state=nx*n_embd=768
                # <2> 第二个值为上方的present张量，其存储着past_key张量与这次迭代的key张量合并后
                # 的新key张量，以及past_value张量与这次迭代的value张量合并后的新value张量，其形状为(2, batch_size,
                # num_head, sql_len+1, head_features)
                # <3> 若output_attentions为True，则第三个值为attn_outputs列表中的注意力分数张
                # 量w
                # <4> 若此时进行了Cross Attention计算，则第四个值为'交叉多头注意力计算结果列表
                # cross_attn_outputs'中的交叉注意力分数张量cross_attention，其形状为(batch_size, num_head, 1,
                # enc_seq_len)
                outputs = block(
                    hidden_states,
                    layer_past=layer_past,
                    attention_mask=attention_mask,
                    head_mask=head_mask[i],
                    encoder_hidden_states=encoder_hidden_states,
                    encoder_attention_mask=encoder_attention_mask,
                    use_cache=use_cache,
                    output_attentions=output_attentions,
                )

            hidden_states = outputs[0]
            if use_cache is True:
                presents = presents + (outputs[1],)

            if output_attentions:
                all_self_attentions = all_self_attentions + (outputs[2],)
                if self.config.add_cross_attention:
                    all_cross_attentions = all_cross_attentions + (outputs[3],)
# 将 GPT-2 模型中 12 层 Block 模块计算后得到的最终 hidden_states 张量再输入
# layerNormalization 层中进行计算
        hidden_states = self.ln_f(hidden_states)

       # output_shape = input_shape + (hidden_states.size(-1),)
        hidden_states = hidden_states.view(*output_shape)

# 将上方最后一层 Block () 循环结束之后得到的结果隐藏状态张量 hidden_states 也添加到元组all_hidden_states 中

        if output_hidden_states:
            all_hidden_states = all_hidden_states + (hidden_states,)
        # 返回的outputs包含以下内容：
        # <1> 第一个值为经过GPT-2模型中12层Block模块计算后得到的最终hidden_states张量
        # <2> 第二个值若use_cache为True，则为存储12个present张量的presents元组，每个present张量
        # 存储着past_key张量与这次迭代的key张量合并后的新key张量，以及past_value张量与这次迭代的value张量合并后
        # 的新value张量
        # <3> 第三个值若output_attentions为True，则为GPT-2模型中12层Block模块计算后得到的
        # 存储12个注意力分数张量w的all_self_attentions元组
        # <4> 第四个值若output_attentions为True且config.add_cross_attention为True，则为
        # 存储12个交叉注意力分数张量cross_attention的all_cross_attentions元组
        # <5> 第五个值若output_hidden_states为True，则为GPT-2模型中12层Block模块计算后得到的
        # 存储12个隐藏状态张量hidden_state的all_hidden_states元组

# 此时返回的元素为
# <1> 第一个值为 GPT-2 模型中经过 12 层 Block 模块计算后得到的最终 hidden_states 张量，形状为
# (batch_size, 1, n_state), all_head_size=n_state=nxn_embd=768
# <2> 第二个值为 GPT-2 模型中 12 层 Block 模块计算后得到的存储 12 个 present 张量的 presents 元组，
# 每一个 present 张量存储着 past_key 张量与这次迭代的 key 张量合并后的新 key 张量，以及 past_value 张量与这次迭代
# 的 value 张量合并后的新 value 张量
# <3> 若 output_hidden_states 为 True, 则第三个值为 GPT-2 模型中 12 层 Block 模块计算后得到的
# 存储 12 个隐藏状态张量 hidden_states 的 all_hidden_states 元组
# <4> 若 output_attentions 为 True 则第四个值为 GPT-2 模型中 12 层 Block 模块计算后得到的存储 12
# 个注意力分数张量 w 的 all_self_attentions 元组
# <5> 若此时进行了 Cross Attention 计算，则第五个值为 GPT-2 模型中 12 层 Block 模块计算后得到的
# 存储 12 个交叉注意力分数张量 cross_attention 的 all_cross_attentions 元组
# 其中每个交叉注意力分数张量cross_attention 形状为 (batch_size, num_head, 1,
# enc_seq_len)
        if not return_dict:
            return tuple(v for v in [hidden_states, presents, all_self_attentions, all_cross_attentions, all_hidden_states] if v is not None)
        return BaseModelOutputWithPastAndCrossAttentions(
            last_hidden_state=hidden_states,
            past_key_values=presents,
            hidden_states=all_hidden_states,
            attentions=all_self_attentions,
            cross_attentions=all_cross_attentions,
        )
```

GPT2Model类中的代码可参考注释来理解。
在GPT2Model类中，模型的主体包含词嵌入层self.wte、绝对位置嵌入层self.wpe、Dropout层self.drop、含有12个Block模块的ModuleList层self.h，以及最后的LayerNormalization层self.ln_f。



GPT2Model类中，会对输入的input_ids张量、token_type_ids张量、position_ids张量、attention_mask张量等进行预处理工作，主要涉及以下内容：
- input_ids张量、token_type_ids张量、position_ids张量经过嵌入层后变为三维的inputs_embeds张量、position_embeds张量、token_type_embeds张量，这三个张量相加即为一开始输入GPT - 2模型中的hidden_states张量。
- 而attention_mask张量则会扩展为四维张量从而完成对注意力分值的修正。然而在文本生成任务中一般不会添加填充特殊符[PAD]，即无须用到attention_mask张量，因此在用GPT - 2模型进行文本生成任务时attention_mask一般为None。

GPT2Model类中最主要的部分便是循环ModuleList层中的12个Block模块和past_key_values元组中的12个layer_past张量进行运算，这部分执行的操作即为GPT - 2模型主体结构部分的运算过程。 
