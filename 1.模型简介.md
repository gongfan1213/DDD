### 第14章 ChatGPT前身——只具有解码器的GPT - 2模型
本章回到自然语言处理中，前面的章节介绍了自然语言处理中编码器与解码器的使用，并结合在一起完成了一个较为重量级的任务——汉字与拼音的翻译任务，如图14 - 1所示。

![image](https://github.com/user-attachments/assets/6e31a2a6-2db9-4e2d-99ba-e8392129e86d)



可以看到，基于编码器与解码器的翻译模型是深度学习较重要的任务之一，其中的编码器本书使用了大量的篇幅来讲解，这也是本书前面章节用来进行文本分类的主要方法之一。

本章主要介绍另一个重要组件——解码器。随着人们对深度学习的研究日趋成熟，了解和认识到只使用解码器来完成模型的生成任务可能会更加有效，并基于此完成了最终的ChatGPT。

本章首先介绍ChatGPT重要的前期版本——GPT - 2的使用，然后通过实战演示介绍这种仅使用解码器的文本生成模型——GPT - 2。


#### 14.1 GPT - 2模型简介

本节内容不需要掌握，读者仅做了解即可。GPT - 2是OpenAI推出的一项基于单纯解码器的深度学习文本生成模型，其在诞生之初就展现出了令人印象深刻的能力，能够撰写连贯而充满激情的文章，其超出了我们预期的当前语言模型所拥有的能力。但是，GPT - 2并不是特别新颖的架构，它的架构与我们在前面的翻译模型中使用的Decoder非常相似，它在大量数据集上进行了训练，因此可以认为它是一种具有庞大输出能力的语言模型。

##### 14.1.1 GPT - 2模型的输入和输出结构——自回归性
首先我们来看GPT - 2模型的输入和输出结构。GPT - 2模型是一种机器学习模型，能够查看句子的一部分并预测下一个单词。最著名的语言模型是智能手机键盘，可根据用户当前输入的内容提示下一个单词。GPT - 2模型也仿照这种输入输出格式，通过对输入的下一个词进行预测从而获得其生成能力，如图14 - 2所示。

![image](https://github.com/user-attachments/assets/20e0a56c-cab0-4054-853a-89df3983a8f2)


从图14 - 2可以看到，GPT - 2模型的工作方式是，一个Token输出之后，这个Token就会被添加到句子的输入中，从而将新的句子变成下一次输出的输入，这种策略被称为自回归性（Auto - Regression），这也是RNN成功的关键之一。

继续深入这种预测方式，我们可以改变其输入格式，从而使得GPT - 2模型具有问答性质的能力。图14 - 3演示了GPT - 2模型进行问答的解决方案，即在头部加上一个特定的Prompt。目前读者可以将其单纯地理解成一个“问题”或者“引导词”，而GPT - 2模型则根据这个Prompt生成后续的文本。

![image](https://github.com/user-attachments/assets/035370ff-1a3b-40b9-bc51-996f28820984)


可以看到，此时的GPT - 2模型是一种新的深度学习模型结构，只采用Decoder Block作为语言模型，抛弃了Encoder，如图14 - 4所示。

![image](https://github.com/user-attachments/assets/f4f08acd-772b-4f8d-b12a-ce7dd6390e02)


##### 14.1.2 GPT - 2模型的PyTorch实现
前面介绍了GPT - 2模型的组成和构成方法，下面将实现基于GPT - 2模型的组成方案。首先我们来看一下GPT - 2模型的基本结构，如图14 - 5所示。

从图14 - 5可以看到，对于GPT - 2来说，实际上就相当于使用了一个单独的解码器模组来完成数据编码和输出的工作，其主要的数据处理部分都是在解码器中完成的，而根据不同的任务需求，最终的任务结果又可以通过设置不同的头部处理器进行完整的处理。

![image](https://github.com/user-attachments/assets/bc5e9eb5-69c6-49c9-b756-27180f5fd1d1)


