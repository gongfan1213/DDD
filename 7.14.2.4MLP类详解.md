### 14.2.4 MLP类详解
GPT - 2模型源码中MLP类的代码如下：
```python
class MLP(nn.Module):
    def __init__(self, n_state, config):  # in MLP: n_state=3072 (4 * n_embd)
        super().__init__()
        # 此时nx=n_embd=768
        # 而n_state实际为inner_dim，即n_state为4 * n_embd，等于3072
        nx = config.n_embd
        # self.c_fc = Conv1D(n_state, nx)相当于全连接层，其将输入张量的最后一个维度的维度大小由nx(768)投影为n_state(3072)，此时n_state=3072
        self.c_fc = Conv1D(n_state, nx)
        # self.c_proj = Conv1D(nx, n_state)相当于全连接层，其将输入张量的最后一个维度的维度大小由n_state(3072)投影为nx(768)，此时n_state=3072
        self.c_proj = Conv1D(nx, n_state)
        # 设置的激活函数
        self.act = ACT2FN[config.activation_function]
        # 残差dropout层进行正则化操作，防止过拟合
        self.dropout = nn.Dropout(config.resid_pdrop)

    def forward(self, x):
        h = self.act(self.c_fc(x))
        h2 = self.c_proj(h)
        return self.dropout(h2)
```
MLP类中的代码可参考注释来理解。
可以看到，GPT - 2模型主体结构的每个Block模块运算过程中都包含Attention模块与MLP模块的运算。MLP类实质上就是一个两层全连接层模块，这里会将Attention类输出的结果hidden_states张量输入MLP类中进行前馈神经网络运算。将MLP类的输出结果再输入残差连接residual_connection之后，GPT - 2模型结构中一个Block模块的运算过程就会结束，之后将会进行下一个Block模块的运算。 
