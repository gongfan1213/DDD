### 14.3 Hugging Face GPT - 2模型的使用与自定义微调

14.2节介绍了GPT - 2模型源码的主要类，包括GPT2LMHeadModel类、GPT2Model类、Block类、Attention类与MLP类的详细代码。本节将讲解Hugging Face GPT - 2模型的使用与自定义数据集的微调。



#### 14.3.1 模型的使用与自定义数据集的微调

下面首先介绍Hugging Face GPT - 2模型的使用。前文介绍BERT的时候提到过Hugging Face模型的使用，在这里我们将直接使用前文讲解过的知识完成GPT - 2模型的下载与使用。

1. **下载和使用Hugging Face GPT - 2模型**

下载和使用现有的已训练好的GPT - 2模型，代码如下：

```python
from transformers import BertTokenizer, GPT2LMHeadModel, TextGenerationPipeline
tokenizer = BertTokenizer.from_pretrained("uer/gpt2-chinese-cluecorpussmall")
model = GPT2LMHeadModel.from_pretrained("uer/gpt2-chinese-cluecorpussmall")
text_generator = TextGenerationPipeline(model, tokenizer)
result = text_generator("从前有座山", max_length=100, do_sample=True)
print(result)
```

结果请读者自行尝试，在这里提醒一下，每次输出的结果都不会相同，原因会在后面的章节中讲到，这里只需要有输出结果即可。

2. **剖析Hugging Face GPT - 2模型**

如果想重新使用Hugging Face的模型继续训练，那么首先需要根据现有的GPT - 2模型重新加载数据进行处理。

一个非常简单的查看模型结构的方法是直接对模型的结构进行打印，代码如下：

```python

import torch

# 注意GPT2LMHeadModel与GPT2Model这2个模型。

# 其区别在于是否加载最终的输出层，也就是下面图14-9中的最后一行lm_head

from transformers import BertTokenizer, GPT2LMHeadModel, TextGenerationPipeline
tokenizer = BertTokenizer.from_pretrained("uer/gpt2-chinese-cluecorpussmall")
model = GPT2LMHeadModel.from_pretrained("uer/gpt2-chinese-cluecorpussmall")
print(model)
```
打印结果如图14 - 9所示。

![image](https://github.com/user-attachments/assets/78382507-4b95-469a-9e5b-357397de2520)


```
(attn_dropout): Dropout(p=0.1, inplace=False)
(resid_dropout): Dropout(p=0.1, inplace=False)
)
(ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
(mlp): GPT2MLP(
  (c_fc): Conv1D()
  (c_proj): Conv1D()
  (act): NewGELUActivation()
  (dropout): Dropout(p=0.1, inplace=False)
)
)
(lm_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
)
(lm_head): Linear(in_features=768, out_features=21128, bias=False)
```
![image](https://github.com/user-attachments/assets/b8a8366a-0caa-4d8f-84cc-dd20c7d5bdcd)


图14 - 9 打印结果

可以看到，这里打印了GPT2LMHeadModel模型的全部内容，最后一层是在输入的Embedding层基础上进行最终分割的，从而使得输出结果与字符索引进行匹配。而我们需要将模型输出与最终的分类层分割，现成的方法就是分别存储model层和最终的lm_head层的架构和参数，代码如下：

```python

import torch
from transformers import BertTokenizer, GPT2LMHeadModel, TextGenerationPipeline
model = GPT2LMHeadModel.from_pretrained("uer/gpt2-chinese-cluecorpussmall")
print(model)
#下面演示如何获取某一层的参数
lm_weight = (model.lm_head.state_dict()["weight"])
torch.save(lm_weight,"./dataset/lm_weight.pth")
```

这里演示了一个非常简单的获取最终层的参数的方法，即根据层的名称提取和保存对应的参数即可。

注意：实际上，我们可以直接使用GPT2LMHeadModel类来获取GPT - 2生成类完整的参数，在这里分开获取的目的是为下一步讲解ChatGPT的强化学习做个铺垫。

对于模型的使用更为简单，Huggingface为我们提供了一个对应的GPT - 2架构模型，代码如下：

```python
from transformers import BertTokenizer, GPT2Model, TextGenerationPipeline
model = GPT2Model.from_pretrained("uer/gpt2-chinese-cluecorpussmall")
```

3. **使用Hugging Face GPT - 2模块构建自定义的GPT - 2模型**

下面使用上文拆解出的GPT - 2模块来构建自定义的GPT - 2模型，相对于前面所学的内容，可以将构建对应的GPT - 2模型看作一个简单的分类识别模型，代码如下：
```python
import torch
from torch.nn.parameter import Parameter
from transformers import BertTokenizer, GPT2Model, TextGenerationPipeline
tokenizer = BertTokenizer.from_pretrained("uer/gpt2-chinese-cluecorpussmall")

class GPT2(torch.nn.Module):
    def __init__(self):
        super().__init__()
        #with model.no_grad():
        self.model = GPT2Model.from_pretrained("uer/gpt2-chinese-cluecorpussmall")
        self.lm_head = torch.nn.Linear(768,21128,bias=False)
        weight = torch.load("./dataset/lm_weight.pth")
        self.lm_head.weight = Parameter(weight)
        self.value_layer = torch.nn.Sequential(torch.nn.Linear(768,1),torch.nn.Tanh(),
torch.nn.Dropout(0.1))

    def forward(self,token_inputs):
        embedding = self.model(token_inputs)
        embedding = embedding["last_hidden_state"]
        embedding = torch.nn.functional.embedding(embedding)
        logits = self.lm_head(embedding)
        return logits
```

4. **自定义数据输入格式**

想要完成对自定义的GPT - 2模型的训练，设置合适的输入和输出函数是必不可少的，在这里我们选用上文的情感分类数据集通过自带的tokenizer对其进行编码处理。此时完整的数据输入如下：
```python
import torch
import numpy as np
from transformers import BertTokenizer, GPT2LMHeadModel, TextGenerationPipeline
tokenizer = BertTokenizer.from_pretrained("uer/gpt2-chinese-cluecorpussmall")

#首先获取情感分类数据
token_list = []
with open("./ChnSentiCorp.txt", mode="r", encoding="UTF-8") as emotion_file:
    for line in emotion_file.readlines():
        line = line.strip().split(",")
        text = "".join(line[1:])
        inputs = tokenizer(text,return_tensors='pt')
        token = input_ids = inputs["input_ids"]
        attention_mask = inputs["attention_mask"]
        for id in token[0]:
            token_list.append(id.item())
```
```python
token_list = torch.tensor(token_list * 5)
#调用标准的数据输入格式
class TextSamplerDataset(torch.utils.data.Dataset):
    def __init__(self, data, seq_len):
        super().__init__()
        self.data = data
        self.seq_len = seq_len

    def __getitem__(self, index):
        #下面的写法是为了遵守GPT-2的数据输入和输出格式而特定的写法
        rand_start = torch.randint(0, self.data.size(0) - self.seq_len, (1,))
        full_seq = self.data[rand_start : rand_start + self.seq_len + 1].long()
        return full_seq[:-1],full_seq[1:]

    def __len__(self):
        return self.data.size(0) // self.seq_len
```
在这里需要说明的是，在定义getitem函数时，需要遵循GPT - 2特定的输入和输出格式而完成特定的格式设置。

#### 14.3.2 基于预训练模型的评论描述微调
下面使用已完成的代码进行评论描述。需要注意的是，因为这里使用的是lm_weight参数，所以需要预先存档。完整的训练代码如下：
```python
import os
os.environ["CUDA_VISIBLE_DEVICES"] = "0"
import torch
from torch import optim
from torch.utils.data import DataLoader
max_length = 128 + 1
batch_size = 2
device = "cuda"
import model

save_path = "./train_model_emo.pth"
glm_model = model.GPT2()
glm_model.to(device)
#glm_model.load_state_dict(torch.load(save_path),strict=False)
optimizer = torch.optim.AdamW(glm_model.parameters(), lr=2e-4)
lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer,T_max = 1200,eta_min=2e-6,last_epoch=-1)
criterion = torch.nn.CrossEntropyLoss()

import get_data_emotion
train_dataset = get_data_emotion.TextSamplerDataset(get_data_emotion.token_list, max_length)
loader = DataLoader(train_dataset,batch_size=batch_size,shuffle=True, num_workers=0,pin_memory=True)

for epoch in range(30):
    pbar = tqdm(loader, total=len(loader))
    for token_inp,token_tgt in pbar:
        token_inp = token_inp.to(device)
        token_tgt = token_tgt.to(device)
        logits = glm_model(token_inp)
        loss = criterion(logits.view(-1,logits.size(-1)),token_tgt.view(-1))
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
        lr_scheduler.step()  # 执行优化器
        pbar.set_description(f"epoch:{epoch +1}, train_loss:{loss.item():.5f}, lr:{lr_scheduler.get_last_lr()[0]*100:.5f}")
    if (epoch + 1) % 2 == 0:
        torch.save(glm_model.state_dict(),save_path)
```

